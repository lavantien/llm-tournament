# openai-api-base: http://localhost:8000/v1
# openai-api-base: https://glhf.chat/api/openai/v1
openai-api-base: https://api.hyperbolic.xyz/v1
api-key:
  - openai=
  # - openai=
  # - openai=
  - anthropic=
  - mistral=
  - codestral=
  - gemini=
  - deepseek=
  - cohere=
  - openrouter=
  - xai=
architect: true
# model: mistral/mistral-large-latest
model: gemini/gemini-exp-1206
# model: gemini/gemini-2.0-flash-exp
# model: deepseek/deepseek-chat
# model: openrouter/google/gemini-exp-1206:free
# model: openrouter/google/gemini-2.0-flash-exp:free
# model: openai/deepseek-ai/DeepSeek-V3
# model: openai/hf:meta-llama/Llama-3.1-405B-Instruct
# model: openai/hf:deepseek-ai/DeepSeek-V2.5-1210
# model: openai/hf:Qwen/Qwen2.5-Coder-32B-Instruct
# model: openai/hf:Qwen/Qwen2.5-72B-Instruct
# model: openai/hf:nvidia/Llama-3.1-Nemotron-70B-Instruct-HF
# model: openai/hf:meta-llama/Llama-3.3-70B-Instruct
edit-format: diff
# editor-model: gemini/gemini-2.0-flash-exp
editor-model: mistral/mistral-large-latest
editor-edit-format: editor-diff
weak-model: mistral/pixtral-small-latest
# weak-model: mistral/mistral-small-latest
# weak-model: codestral/codestral-latest
read:
  - SYSTEM_PROMPT.md
  - PRELIMINARY_DESIGN.md
yes-always: true
map-tokens: 2048
timeout: 300
cache-prompts: true
cache-keepalive-pings: 300
verbose: false
pretty: true
vim: true
editor: vim
multiline: false
code-theme: coffee
show-diffs: true
