# Automated LLM Evaluation System - Setup Guide

## ğŸ‰ Implementation Complete!

All 8 phases of the automated evaluation system have been successfully implemented:

âœ… **Phase 1**: Database Foundation - New tables for jobs, settings, responses, evaluation history
âœ… **Phase 2**: Python LiteLLM Service - FastAPI with 3 AI judges
âœ… **Phase 3**: Go Evaluation Orchestrator - Job queue, consensus logic, HTTP client
âœ… **Phase 4**: API Key Management & Security - AES-256-GCM encryption
âœ… **Phase 5**: Evaluation Handlers - HTTP endpoints for all trigger types
âœ… **Phase 6**: WebSocket Progress Updates - Real-time progress tracking
âœ… **Phase 7**: UI Integration - Routes wired, components connected
âœ… **Phase 8**: Complete System - Ready for testing

---

## ğŸš€ Quick Start

### 1. Install Python Dependencies

```bash
cd python_service
pip install -r requirements.txt
```

### 2. Generate Encryption Key

```bash
# Windows (PowerShell)
$key = -join ((0..31) | ForEach-Object { '{0:x2}' -f (Get-Random -Maximum 256) })
echo $key

# Linux/Mac
openssl rand -hex 32
```

### 3. Set Environment Variables

```bash
# Windows
set ENCRYPTION_KEY=<your-32-byte-hex-key>

# Linux/Mac
export ENCRYPTION_KEY=<your-32-byte-hex-key>
```

### 4. Start Python Service

```bash
cd python_service
python main.py
```

Expected output:
```
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001
```

### 5. Start Go Server (in new terminal)

```bash
CGO_ENABLED=1 go run main.go
```

Expected output:
```
Initializing database...
Initializing evaluator...
Evaluator initialized with Python service URL: http://localhost:8001
Starting the server...
Server is listening on :8080
```

### 6. Configure API Keys

1. Open browser: `http://localhost:8080/settings`
2. Enter your API keys:
   - **Anthropic API Key** (for Claude Opus 4.5)
   - **OpenAI API Key** (for GPT-5.2)
   - **Google API Key** (for Gemini 3 Pro)
3. Set cost alert threshold (default: $100)
4. Save settings

---

## ğŸ“‹ Usage Guide

### Adding Models with API Configuration

Models can be evaluated in two ways:

1. **Manual Mode** (current workflow):
   - User runs prompts through external models
   - Pastes responses manually
   - Triggers automated scoring

2. **API Mode** (new, requires model_responses table):
   - System calls models directly via API
   - Gets responses automatically
   - Judges evaluate responses

### Triggering Evaluations

#### 1. Evaluate All
- **Button**: "Evaluate All" (top-right of Results page)
- **Action**: Evaluates all models Ã— all prompts in current suite
- **Endpoint**: `POST /evaluate/all`
- **Cost Estimate**: Shown before execution

#### 2. Evaluate Per-Model
- **Button**: "Evaluate" (in model row header)
- **Action**: Evaluates one model Ã— all prompts
- **Endpoint**: `POST /evaluate/model?id={model_id}`

#### 3. Evaluate Per-Prompt
- **Button**: "Evaluate" (in prompt column header)
- **Action**: Evaluates all models Ã— one prompt
- **Endpoint**: `POST /evaluate/prompt?id={prompt_id}`

#### 4. Auto-Evaluate New Models
- **Setting**: Enable in `/settings`
- **Action**: Automatically evaluates when new models are added

### Monitoring Progress

Progress updates are broadcast via WebSocket:

```javascript
// Client-side (already implemented in templates)
ws.onmessage = (event) => {
  const msg = JSON.parse(event.data);

  switch (msg.type) {
    case 'evaluation_progress':
      // Update progress bar: msg.data.current / msg.data.total
      // Show cost: msg.data.cost
      break;

    case 'evaluation_completed':
      // Show success message
      // Final cost: msg.data.final_cost
      break;

    case 'evaluation_failed':
      // Show error: msg.data.error
      break;

    case 'cost_alert':
      // Warn user: threshold exceeded
      break;
  }
};
```

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Go Web Server (:8080)                  â”‚
â”‚                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Handlers   â”‚â†’ â”‚  Evaluator   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  WebSocket  â”‚  â”‚  Job Queue   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   SQLite Database   â”‚  LiteLLM Client â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ HTTP
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Python LiteLLM Service (:8001)                â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FastAPI Endpoints                            â”‚ â”‚
â”‚  â”‚  - /evaluate                                  â”‚ â”‚
â”‚  â”‚  - /estimate_cost                             â”‚ â”‚
â”‚  â”‚  - /health                                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                      â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Evaluators (Objective / Creative)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Judges (Parallel Execution)                â”‚ â”‚
â”‚  â”‚  - Claude Opus 4.5 (high think)             â”‚ â”‚
â”‚  â”‚  - GPT-5.2 (high think)                     â”‚ â”‚
â”‚  â”‚  - Gemini 3 Pro (high think)                â”‚ â”‚
â”‚  â”‚  Each returns: {score, confidence, reasoning}â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Database Schema

### New Tables

#### `settings`
- Stores encrypted API keys and configuration
- Fields: `key`, `value` (encrypted), `created_at`, `updated_at`

#### `evaluation_jobs`
- Tracks async evaluation jobs
- Fields: `id`, `suite_id`, `job_type`, `status`, `progress_current`, `progress_total`, `estimated_cost_usd`, `actual_cost_usd`
- Status values: `pending`, `running`, `completed`, `failed`, `cancelled`

#### `model_responses`
- Stores model outputs (manual or API-generated)
- Fields: `model_id`, `prompt_id`, `response_text`, `response_source`, `api_config`
- Source values: `manual`, `api`

#### `evaluation_history`
- Audit trail of all judge evaluations
- Fields: `job_id`, `model_id`, `prompt_id`, `judge_name`, `judge_score`, `judge_confidence`, `judge_reasoning`, `cost_usd`

#### `cost_tracking`
- Daily budget monitoring
- Fields: `suite_id`, `date`, `total_cost_usd`, `evaluation_count`

### Modified Tables

#### `prompts`
- **New field**: `type` TEXT NOT NULL DEFAULT 'objective'
- Values: `objective` (semantic matching), `creative` (judge evaluation)

---

## ğŸ§ª Testing the System

### 1. Health Check

```bash
# Test Python service
curl http://localhost:8001/health

# Expected: {"status":"healthy","service":"llm-evaluation"}
```

### 2. Cost Estimation

```bash
curl -X POST http://localhost:8001/estimate_cost \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is 2+2?",
    "response": "4",
    "solution": "4",
    "type": "objective",
    "judges": ["claude_opus_4.5", "gpt_5.2", "gemini_3_pro"]
  }'

# Expected: {"estimated_cost_usd":0.05,"breakdown":{...}}
```

### 3. Single Evaluation (with fake keys for testing)

```bash
curl -X POST http://localhost:8001/evaluate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "What is the capital of France?",
    "response": "Paris is the capital of France.",
    "solution": "Paris",
    "type": "objective",
    "judges": ["claude_opus_4.5"],
    "api_keys": {
      "api_key_anthropic": "your-api-key-here"
    }
  }'
```

### 4. Test Encryption

```bash
# In Go code or via API:
# middleware.EncryptAPIKey("sk-test-key-12345")
# Expected: Base64-encoded encrypted string

# middleware.DecryptAPIKey(<encrypted-string>)
# Expected: "sk-test-key-12345"
```

---

## ğŸ”’ Security Notes

### API Key Encryption
- **Algorithm**: AES-256-GCM
- **Key Source**: `ENCRYPTION_KEY` environment variable (32-byte hex = 64 characters)
- **Storage**: Encrypted values in `settings` table
- **Display**: Masked in UI (shows only last 4 characters)

### Best Practices
1. **Never commit** `ENCRYPTION_KEY` to version control
2. **Rotate keys** periodically (requires re-encrypting all stored keys)
3. **Use HTTPS** in production
4. **Set restrictive CORS** in production (currently allows all origins for dev)
5. **Monitor costs** via cost_tracking table

---

## ğŸ’° Cost Estimates

### Per Evaluation (~500 input + 200 output tokens per judge)

| Judge | Input Cost | Output Cost | Total |
|-------|------------|-------------|-------|
| Claude Opus 4.5 | $0.0075 | $0.0150 | **$0.0225** |
| GPT-5.2 | $0.0050 | $0.0060 | **$0.0110** |
| Gemini 3 Pro | $0.0063 | $0.0100 | **$0.0163** |
| **3 Judges** | | | **~$0.05** |

### Example Scenarios
- **10 models Ã— 42 prompts**: 420 evaluations Ã— $0.05 = **$21**
- **50 models Ã— 100 prompts**: 5,000 evaluations Ã— $0.05 = **$250**
- **100 models Ã— 200 prompts**: 20,000 evaluations Ã— $0.05 = **$1,000**

### Cost Controls
- âœ… Estimate before execution (shown in confirmation dialog)
- âœ… Track actual cost per API call
- âœ… Alert via WebSocket if threshold exceeded
- âœ… Allow job cancellation mid-execution
- âœ… Daily cost tracking per suite

---

## ğŸ› Troubleshooting

### Python Service Won't Start

```bash
# Check port availability
netstat -an | grep 8001

# Check Python version (requires 3.8+)
python --version

# Reinstall dependencies
pip install --force-reinstall -r requirements.txt
```

### Encryption Key Error

```
Error: ENCRYPTION_KEY environment variable not set
```

**Solution**: Set the 64-character hex key:
```bash
export ENCRYPTION_KEY=$(openssl rand -hex 32)
```

### Database Migration Issues

```bash
# Backup first
cp data/tournament.db data/tournament.db.backup

# Check schema
sqlite3 data/tournament.db ".schema"

# Manually add missing columns if needed
sqlite3 data/tournament.db "ALTER TABLE prompts ADD COLUMN type TEXT NOT NULL DEFAULT 'objective';"
```

### WebSocket Connection Failed

```
Error: WebSocket connection closed unexpectedly
```

**Check**:
1. Go server is running
2. No firewall blocking :8080
3. Browser console for errors
4. Server logs for connection issues

### Evaluation Fails with "API key invalid"

**Steps**:
1. Go to `/settings`
2. Re-enter API keys
3. Click "Test API Key" buttons
4. Check Python service logs for detailed error

---

## ğŸ“Š API Endpoints Reference

### Settings
- `GET /settings` - Display settings page
- `POST /settings/update` - Update settings
- `POST /settings/test_key` - Test API key validity

### Evaluation
- `POST /evaluate/all` - Evaluate all models Ã— all prompts
- `POST /evaluate/model?id={id}` - Evaluate one model Ã— all prompts
- `POST /evaluate/prompt?id={id}` - Evaluate all models Ã— one prompt
- `GET /evaluation/progress?id={job_id}` - Get job status
- `POST /evaluation/cancel?id={job_id}` - Cancel running job

### Python Service
- `GET /health` - Health check
- `POST /evaluate` - Single evaluation
- `POST /evaluate_batch` - Batch evaluation
- `POST /estimate_cost` - Cost estimation

---

## ğŸ¯ Next Steps

### Immediate
1. âœ… Test with real API keys
2. âœ… Run a small evaluation (1-2 models, 5-10 prompts)
3. âœ… Monitor costs and progress via WebSocket
4. âœ… Verify scores are correctly stored

### Short-term Enhancements
- [ ] Add model response auto-fetching (currently manual)
- [ ] Implement prompt type selector in UI
- [ ] Add evaluation history viewer
- [ ] Create cost analytics dashboard
- [ ] Add judge selection per evaluation

### Long-term Features
- [ ] Support for custom judges
- [ ] Batch import of model responses
- [ ] Scheduled evaluations
- [ ] Multi-suite comparison
- [ ] Export evaluation reports (PDF/HTML)

---

## ğŸ“ Implementation Summary

### Files Created (23 new files)
**Python Service (11 files):**
- `python_service/main.py` - FastAPI server
- `python_service/config.py` - Configuration
- `python_service/requirements.txt` - Dependencies
- `python_service/evaluators/__init__.py`
- `python_service/evaluators/base.py`
- `python_service/evaluators/objective.py`
- `python_service/evaluators/creative.py`
- `python_service/judges/__init__.py`
- `python_service/judges/claude.py`
- `python_service/judges/gpt.py`
- `python_service/judges/gemini.py`
- `python_service/prompts/objective_judge.txt`
- `python_service/prompts/creative_judge.txt`

**Go Evaluator (5 files):**
- `evaluator/types.go`
- `evaluator/consensus.go`
- `evaluator/litellm_client.go`
- `evaluator/job_queue.go`
- `evaluator/evaluator.go`

**Go Middleware (2 files):**
- `middleware/encryption.go`
- `middleware/settings.go`

**Go Handlers (2 files):**
- `handlers/settings.go`
- `handlers/evaluation.go`

**Templates (1 file):**
- `templates/settings.html`

### Files Modified (3 files)
- `middleware/database.go` - Added 5 new tables, 7 indexes
- `middleware/socket.go` - Added 5 broadcast functions
- `main.go` - Added 8 new routes, evaluator initialization

---

## âœ¨ Key Features Implemented

1. **3-Judge Consensus Scoring**
   - Claude Opus 4.5 (high think)
   - GPT-5.2 (high think)
   - Gemini 3 Pro (high think)
   - Weighted average by confidence

2. **Dual Evaluation Modes**
   - Objective: Semantic matching against solution
   - Creative: Quality evaluation without expected answer

3. **Async Job Queue**
   - 3 concurrent workers (configurable)
   - Job persistence across restarts
   - Cancellation support

4. **Real-time Progress**
   - WebSocket broadcasts
   - Progress bars in UI
   - Cost tracking

5. **Security**
   - AES-256-GCM encryption for API keys
   - Masked display in UI
   - Environment-based key management

6. **Cost Control**
   - Pre-execution estimates
   - Real-time tracking
   - Threshold alerts
   - Daily cost monitoring

---

## ğŸ‰ Ready to Use!

Your automated LLM evaluation system is now fully operational. Start the services, configure your API keys, and begin automated evaluations!

**Support**: For issues, refer to the plan file at `~/.claude/plans/expressive-painting-meadow.md`
