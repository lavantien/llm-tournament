# ğŸ† LLM Tournament Arena

[![Go Version](https://img.shields.io/badge/Go-1.24+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![Python Version](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat&logo=python&logoColor=white)](https://python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![SQLite](https://img.shields.io/badge/SQLite-003B57?style=flat&logo=sqlite&logoColor=white)](https://sqlite.org/)
[![Coverage](https://img.shields.io/badge/Coverage-79.1%25-yellowgreen.svg)]()

**A comprehensive benchmarking platform for evaluating and comparing Large Language Models**

ğŸ“¦ **Single Binary Deployment** â€¢ âš¡ **WebSocket Real-Time Updates** â€¢ ğŸ“Š **Interactive Dashboards** â€¢ ğŸ¤– **AI-Powered Evaluation**

---

## ğŸ“‘ Table of Contents

- [Quick Start](#-quick-start)
- [Key Features](#-key-features)
- [Architecture](#ï¸-architecture)
- [Tech Stack](#ï¸-tech-stack)
- [Complementary Tools](#-complementary-tools)
- [Getting Started](#-getting-started)
- [Testing](#-testing)
- [API Reference](#-api-reference)
- [Project Structure](#-project-structure)
- [Environment Variables](#-environment-variables)
- [Usage Guide](#-usage-guide)
- [Advanced Features](#-advanced-features)
- [Contributing](#-contributing)
- [Roadmap](#-roadmap)
- [License](#-license)
- [Contact](#-contact)

<details>
    <summary>Program Screenshots (expand)</summary>

UI Results:
![UI Results](./assets/ui-results.png)
UI Evaluate:
![UI Evaluate](./assets/ui-evaluate.png)
UI Stats:
![UI Stats](./assets/ui-stats.png)
UI Prompts:
![UI Prompts](./assets/ui-prompts.png)
UI Edit Prompt:
![UI Edit Prompt](./assets/ui-edit-prompt.png)
UI Profiles:
![UI Profiles](./assets/ui-profiles.png)

</details>

## ğŸš€ Quick Start

```bash
# Clone & Run
git clone https://github.com/lavantien/llm-tournament.git
cd llm-tournament
make setenv
make migrate
make dedup
make run
```

Access at `http://localhost:8080`

## ğŸŒŸ Key Features

### ğŸ§ª **Evaluation Engine**
- ğŸ¯ Real-time scoring with WebSocket updates on a 0-100 scale (scored in increments: 0, 20, 40, 60, 80, 100)
- ğŸ“ˆ Automatic model ranking with live leaderboard updates
- ğŸ§® Granular scoring system with state backup and rollback (restore "Previous" state)
- ğŸ”„ Instant propagation of score changes to all connected clients via WebSockets
- ğŸ”€ Random mock score generation using weighted tiers for prototyping

### ğŸ¤– **Automated LLM Evaluation** âœ¨ NEW!
- ğŸ§  **Multi-Judge Consensus Scoring**: 3 AI judges evaluate responses in parallel
  - Claude Opus 4.5 (extended thinking)
  - GPT-5.2 (extended thinking)
  - Gemini 3 Pro (extended thinking)
- âš–ï¸ **Weighted Consensus Algorithm**: Scores aggregated by judge confidence levels
- ğŸ­ **Dual Evaluation Modes**:
  - **Objective**: Semantic matching against expected solutions
  - **Creative**: Quality evaluation without predefined answers
- ğŸ“Š **Real-Time Progress Tracking**: WebSocket broadcasts for job status, progress, and costs
- ğŸ’° **Cost Management**: Pre-execution estimates, real-time tracking, threshold alerts (~$0.05 per evaluation)
- âš¡ **Async Job Queue**: 3 concurrent workers with job persistence across restarts
- ğŸ” **Secure API Key Storage**: AES-256-GCM encrypted credentials
- ğŸ¯ **Flexible Triggers**: Evaluate all, per-model, per-prompt, or auto-evaluate new models
- ğŸ“ˆ **Evaluation History**: Complete audit trail with judge reasoning and confidence scores

### ğŸ“š **Prompt Suite & Test Management**
- ğŸ—‚ï¸ Create, rename, select, and delete independent prompt suites
- ğŸ”— Isolated profiles, prompts, and results per suite for organized evaluations
- âš¡ One-click suite switching with instantaneous UI updates
- ğŸ“¦ JSON import/export for prompt suites and evaluation results
- ğŸ§© Integrity features including duplicate prompt cleanup and migration support from JSON to SQLite

### âœï¸ **Prompt Workshop**
- ğŸ“ Rich Markdown editor with live preview for crafting prompts and solutions
- ğŸ–‡ï¸ Assign reusable evaluation profiles to prompts for categorization
- ğŸ” Advanced search and multi-criteria filtering within prompts
- ğŸšï¸ Intuitive drag-and-drop reordering and bulk operations (selection, deletion, export)
- ğŸ“‹ One-click copy-to-clipboard functionality for prompt text

### ğŸ¤– **Model Arena**
- â• Seamless addition of new models with automatic score initialization
- âœï¸ In-place model renaming while preserving existing scores and results
- ğŸ—‘ï¸ Model removal with confirmation to maintain data integrity
- ğŸ“Š Dynamic, color-coded scoring visualization with real-time updates
- ğŸ” Advanced model search and filtering to compare performance effectively
- ğŸ² Random mock score generation with weighted distribution reflecting performance tiers

### ğŸ‘¤ **Profile System**
- ğŸ“‹ Creation of reusable evaluation profiles with descriptive Markdown support
- ğŸ”– Automatic updating of associated prompts when profiles are renamed
- ğŸ” Profile-based filtering in prompt views to focus on specific categories
- ğŸ“ Live preview of profile descriptions for intuitive setup

### ğŸ“Š **Analytics & Tier Insights**
- ğŸ“Š Detailed score breakdowns powered by Chart.js with interactive visualizations
- ğŸ† Comprehensive tier classification based on total scores:
  - Transcendental (â‰¥3780)
  - Cosmic (3360â€“3779)
  - Divine (2700â€“3359)
  - Celestial (2400â€“2699)
  - Ascendant (2100â€“2399)
  - Ethereal (1800â€“2099)
  - Mystic (1500â€“1799)
  - Astral (1200â€“1499)
  - Spiritual (900â€“1199)
  - Primal (600â€“899)
  - Mortal (300â€“599)
  - Primordial (<300)
- ğŸ“ˆ Visualization of score distributions and tier-based model grouping
- ğŸ“‘ Interactive performance comparisons across evaluated models

### ğŸ’» **Evaluation Interface**
- ğŸ¯ Streamlined scoring with color-coded buttons
- ğŸ“ Full prompt and solution display with Markdown rendering
- â¬…ï¸â¡ï¸ Previous/Next navigation between prompts
- ğŸ“‹ One-click copying of raw prompt text
- ğŸ” Clear visualization of current scores
- ğŸƒâ€â™‚ï¸ Rapid evaluation workflow

### ğŸ”„ **Real-Time Collaboration**
- ğŸŒ WebSocket-based instant updates across all clients
- ğŸ“¤ Simultaneous editing with conflict resolution
- ğŸ”„ Broadcast of all changes to connected users
- ğŸ“¡ Connection status monitoring
- ğŸ”„ Automatic reconnection handling

## ğŸ—ï¸ Architecture

```
Go Server (:8080)                Python Service (:8001)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ HTTP Handlers â”‚                â”‚  AI Judge Service   â”‚
â”‚ WebSocket Hub â”‚ â”€â”€â”€â”€ HTTP â”€â”€â”€â†’ â”‚  (3 LLM judges)     â”‚
â”‚ Job Queue     â”‚                â”‚  Consensus scoring  â”‚
â”‚ SQLite DB     â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Request Flow**: User â†’ Handlers â†’ Middleware â†’ SQLite â†’ WebSocket Broadcast
**Evaluation**: Job Queue â†’ Python Service â†’ AI Judges â†’ Consensus â†’ Score Update

## ğŸ› ï¸ Tech Stack

| Layer | Technologies |
|-------|-------------|
| **Backend** | Go 1.24+, Gorilla WebSocket, Blackfriday, Bluemonday, SQLite, AES-256-GCM |
| **AI Service** | Python 3.8+, FastAPI, LiteLLM, Anthropic/OpenAI/Google SDKs |
| **Frontend** | HTML5, CSS3, JavaScript ES6+, Chart.js 4.x, Marked.js |
| **Data** | SQLite, JSON import/export, State versioning, Encrypted settings |
| **Security** | XSS sanitization, CORS protection, Input validation, Encrypted API keys |

## ğŸ§° Complementary Tools

| Tool | Path | Description |
|------|------|-------------|
| **TTS** | `tools/tts/podcast.py` | Generate podcast audio using Kokoro ONNX models |
| **Image BG Removal** | `tools/bg_batch_eraser/main.py` | Remove backgrounds using BEN2 model |
| **Video BG Removal** | `tools/bg_batch_eraser/vidseg.py` | Extract foreground with alpha channel |
| **Claude Pipe** | `tools/openwebui/pipes/anthropic_claude_thinking_96k.py` | OpenWebUI pipe for Claude (96k context) |
| **RAG Agent** | `tools/ragweb_agent/` | RAG capabilities for web content |

## ğŸ Getting Started

### Prerequisites
- Go 1.24+
- Python 3.8+ (for automated evaluation)
- Make
- Git
- SQLite
- GCC

### Installation & Running

#### Manual Evaluation (Traditional)
```bash
# Development mode
./dev.sh

# Setup
make setenv
make migrate
make dedup
make run

# Production build
make build
./release/llm-tournament
```

#### Automated Evaluation (NEW!)
```bash
# 1. Install Python dependencies
cd python_service
pip install -r requirements.txt

# 2. Generate and set encryption key
export ENCRYPTION_KEY=$(openssl rand -hex 32)  # Linux/Mac
# OR
set ENCRYPTION_KEY=<generated-key>             # Windows

# 3. Start Python evaluation service
python main.py  # Runs on :8001

# 4. Start Go server (in new terminal)
cd ..
CGO_ENABLED=1 go run main.go  # Runs on :8080

# 5. Configure API keys at http://localhost:8080/settings
```

**ğŸ“– Complete Setup Guide**: See [AUTOMATED_EVALUATION_SETUP.md](AUTOMATED_EVALUATION_SETUP.md)

## ğŸ§ª Testing

```bash
# Run all tests with TDD-guard, race detection, and coverage
make test

# Run tests with verbose output (bypasses TDD-guard)
make test-verbose

# Manual test run
CGO_ENABLED=1 go test ./... -v -race -cover

# Run specific package tests
CGO_ENABLED=1 go test ./handlers/... -v

# Test Python service health
curl http://localhost:8001/health
```

## ğŸ“¡ API Reference

### Evaluation Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/evaluate/all` | Evaluate all models Ã— all prompts |
| `POST` | `/evaluate/model?id={id}` | Evaluate one model Ã— all prompts |
| `POST` | `/evaluate/prompt?id={id}` | Evaluate all models Ã— one prompt |
| `GET` | `/evaluation/progress?id={job_id}` | Get job status |
| `POST` | `/evaluation/cancel?id={job_id}` | Cancel running job |

### Settings Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/settings` | Settings page |
| `POST` | `/settings/update` | Update settings |
| `POST` | `/settings/test_key` | Test API key validity |

### Core Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/prompts` | Prompts list (default route) |
| `GET` | `/results` | Results and scoring |
| `GET` | `/stats` | Analytics and tier insights |
| `GET` | `/profiles` | Profile management |
| `WS` | `/ws` | WebSocket connection |

## ğŸ“ Project Structure

```
llm-tournament/
â”œâ”€â”€ main.go                 # Entry point, routing, server setup
â”œâ”€â”€ handlers/               # HTTP request handlers
â”‚   â”œâ”€â”€ evaluation.go       # Automated evaluation triggers
â”‚   â”œâ”€â”€ settings.go         # API key management
â”‚   â”œâ”€â”€ models.go           # Model CRUD
â”‚   â”œâ”€â”€ prompt.go           # Prompt operations
â”‚   â”œâ”€â”€ results.go          # Results display, scoring
â”‚   â”œâ”€â”€ stats.go            # Analytics, tier classification
â”‚   â”œâ”€â”€ profiles.go         # Profile management
â”‚   â”œâ”€â”€ suites.go           # Suite management
â”‚   â””â”€â”€ *_test.go           # 8 test files (9,060 lines)
â”œâ”€â”€ middleware/             # Business logic, data layer
â”‚   â”œâ”€â”€ database.go         # SQLite schema, migrations
â”‚   â”œâ”€â”€ socket.go           # WebSocket handling
â”‚   â”œâ”€â”€ encryption.go       # AES-256-GCM for API keys
â”‚   â”œâ”€â”€ settings.go         # Settings CRUD
â”‚   â”œâ”€â”€ state.go            # Data models
â”‚   â””â”€â”€ *_test.go           # 9 test files (5,453 lines)
â”œâ”€â”€ evaluator/              # Automated LLM evaluation
â”‚   â”œâ”€â”€ evaluator.go        # Main orchestrator
â”‚   â”œâ”€â”€ job_queue.go        # Async job queue (3 workers)
â”‚   â”œâ”€â”€ litellm_client.go   # Python service client
â”‚   â”œâ”€â”€ consensus.go        # Score consensus logic
â”‚   â”œâ”€â”€ types.go            # Data types
â”‚   â””â”€â”€ *_test.go           # 4 test files (2,904 lines)
â”œâ”€â”€ python_service/         # AI Judge service
â”‚   â”œâ”€â”€ main.py             # FastAPI server
â”‚   â”œâ”€â”€ evaluators/         # Evaluation strategies
â”‚   â”œâ”€â”€ judges/             # Claude/GPT/Gemini implementations
â”‚   â””â”€â”€ prompts/            # Judge prompt templates
â”œâ”€â”€ templates/              # HTML, CSS, JavaScript
â””â”€â”€ data/                   # SQLite database

Test Coverage: 79.1% (17,417 lines of tests)
```

## ğŸ” Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `CGO_ENABLED` | Yes | Set to `1` (required for SQLite) |
| `ENCRYPTION_KEY` | For evaluation | 64-char hex for API key encryption: `openssl rand -hex 32` |

See [AUTOMATED_EVALUATION_SETUP.md](AUTOMATED_EVALUATION_SETUP.md) for detailed configuration.

## ğŸ“š Usage Guide

### Manual Evaluation Workflow

1. **Set Up Test Suites**
   - Create a new suite for your evaluation task
   - Configure profiles for different prompt categories
   - Import existing prompts or create new ones

2. **Configure Models**
   - Add each model you want to evaluate
   - Models can represent different LLMs, versions, or configurations

3. **Prepare Prompts**
   - Write prompts with appropriate solutions
   - Set prompt type: `objective` (with expected answer) or `creative` (open-ended)
   - Assign profiles for categorization
   - Arrange prompts in desired evaluation order

4. **Run Evaluations (Manual)**
   - Navigate through prompts and assess each model
   - Use the 0-5 scoring system (0, 20, 40, 60, 80, 100 points)
   - Copy prompts directly to your LLM for testing

5. **Analyze Results**
   - View the results page for summary scores
   - Examine tier classifications in the stats page
   - Compare performance across different prompt types
   - Export results for external analysis

### Automated Evaluation Workflow âœ¨ NEW!

1. **Configure API Keys**
   - Navigate to `/settings`
   - Enter API keys for Claude, GPT, and Gemini
   - Set cost alert threshold (default: $100)
   - Enable auto-evaluate for new models (optional)

2. **Prepare Prompts**
   - Set prompt type: `objective` or `creative`
   - For objective prompts: Add expected solution for semantic matching
   - For creative prompts: Judges evaluate quality without expected answer

3. **Trigger Automated Evaluation**
   - **Evaluate All**: Click "Evaluate All" button (all models Ã— all prompts)
   - **Per-Model**: Click "Evaluate" on model row (one model Ã— all prompts)
   - **Per-Prompt**: Click "Evaluate" on prompt column (all models Ã— one prompt)
   - **Auto**: Enable auto-evaluate in settings for new models

4. **Monitor Progress**
   - Real-time WebSocket updates show progress
   - View current/total evaluations and running cost
   - Cancel job anytime if needed

5. **Review Results**
   - Consensus scores automatically saved
   - View evaluation history with judge reasoning
   - Check cost tracking per suite
   - Compare judge confidence levels

## ğŸ”§ Advanced Features

<details>
<summary><b>Manual Evaluation</b> (click to expand)</summary>

- Bulk operations (select, delete, export)
- Drag-and-drop prompt reordering
- State backup/restore ("Previous" button)
- Mock score generation for testing
- Advanced search and filtering
- JSON migration with duplicate cleanup
</details>

<details>
<summary><b>Automated Evaluation</b> (click to expand)</summary>

- 3 concurrent workers with job persistence
- Pre-execution cost estimation
- Re-evaluation with confirmation
- Configurable AI judges
- Complete evaluation history with reasoning
- Daily budget monitoring and alerts
- Job cancellation support
</details>

## ğŸ¤ Contributing

We welcome contributions! First time? Try issues labeled `good first issue`.

```bash
# Fork, clone, and create feature branch
git checkout -b feature/your-feature

# Make changes, run tests
make test

# Submit PR with description
```

**Areas needing help:** Evaluation workflows, storage backends, visualization, CI/CD

## ğŸ“œ License

MIT License - See [LICENSE](LICENSE) for details

## ğŸ“¬ Contact

Email: [cariyaputta@gmail.com](mailto:cariyaputta@gmail.com)
