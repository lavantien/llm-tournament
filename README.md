# ğŸ† LLM Tournament Arena

**A dynamic evaluation platform for benchmarking Large Language Models**  
*Real-time scoring â€¢ Modular test suites â€¢ Collaborative evaluation â€¢ Granular analytics*

ğŸ“¦ **Single Binary Deployment** â€¢ âš¡ **WebSocket Real-Time Updates** â€¢ ğŸ“Š **Interactive Dashboards**

<details>
    <summary>Preview Pictures (expand)</summary>

UI Result Page Preview:
![UI Result Page Preview](./assets/ui-result-page.png)

UI Prompt Manager Preview:
![UI Prompt Manager Preview](./assets/ui-prompt-manager.png)

UI Profile Manager Preview:
![UI Profile Manager Preview](./assets/ui-profile-manager.png)

UI Stats Page Preview:
![UI Stats Page Preview](./assets/ui-stats-page.png)

UI Prompt Edit Preview:
![UI Prompt Edit Preview](./assets/ui-prompt-edit.png)

</details>

## ğŸš€ Quick Start

```bash
# Clone & Run
git clone https://github.com/lavantien/llm-tournament.git
cd llm-tournament
make run
```

Access at `http://localhost:8080`

## ğŸŒŸ Key Features

### ğŸ§ª **Evaluation Engine**
- ğŸ¯ Real-time scoring with WebSocket updates
- ğŸ“ˆ Automatic model ranking & tier classification
- ğŸ§® Score normalization (0-100 scale)
- ğŸ“‰ Pass percentage calculations
- ğŸ”„ Live leaderboard updates

### ğŸ“š **Test Suite Management**
- ğŸ—‚ï¸ Create/rename/delete prompt suites
- ğŸ”— Associated profiles & results per suite
- âš¡ One-click suite switching
- ğŸ“¦ Suite import/export (JSON)
- ğŸ·ï¸ Profile-based prompt categorization

### âœï¸ **Prompt Workshop**
- ğŸ“ Rich text editing with Markdown support
- ğŸ–‡ï¸ Profile associations for prompts
- ğŸ§© Bulk operations (delete/export)
- ğŸšï¸ Drag-and-drop reordering
- ğŸ” Advanced search & filtering
- ğŸ“¤ CSV/JSON import/export

### ğŸ¤– **Model Arena**
- â• Add/remove evaluation models
- âœï¸ Model renaming
- ğŸ“Š Side-by-side comparisons
- ğŸ… Tier-based ranking system
- ğŸ“¦ Result snapshot archiving

### ğŸ“Š **Analytics Suite**
- ğŸ“Š Interactive score breakdowns
- ğŸ† Tier classification system:
  - Transcendent (1900-2000) ğŸŒŒ
  - Grandmaster (1700-1899) ğŸ¥‡
  - Pro Player (1200-1399) ğŸ®
  - Beginner (0-599) ğŸ£
- ğŸ“ˆ Historical trend visualization
- ğŸ“‰ Model performance heatmaps
- ğŸ“Œ Pin notable evaluations

### ğŸ‘¥ **Collaboration Tools**
- ğŸ”„ Real-time multiplayer updates
- ğŸ“¤ Shared result exports
- ğŸ’¬ Comment threads
- ğŸ·ï¸ Evaluation tagging
- ğŸ“… Session history

## ğŸ› ï¸ Tech Stack

**Backend**  
`Go 1.21+` â€¢ `Gorilla WebSocket` â€¢ `Blackfriday` â€¢ `Bluemonday`

**Frontend**  
`HTML5` â€¢ `CSS3` â€¢ `JavaScript ES6+` â€¢ `Chart.js`

**Data**  
`JSON Storage` â€¢ `File-based State` â€¢ `JSON Import/Export`

**Security**  
`XSS Sanitization` â€¢ `CORS Protection` â€¢ `Rate Limiting`

## ğŸ Getting Started

### Prerequisites
- Go 1.21+
- Node.js 16+ (for asset building)
- Make
- [Aider](https://aider.chat/)
- Aider's configs from my [dotfiles](https://github.com/lavantien/dotfiles)

### Development
- Read through the Aider's usage guide: <https://aider.chat/docs/usage.html>
- Aider with o3-mini (high) as architect and claude-3.7-sonnet as editor 
```bash
# Development 
aider --no-gitignore --watch-files

# Production
make build
./release/llm-tournament
```

## ğŸ“š Usage Guide

1. **Create Test Suite**
   - Navigate to `Suites â†’ New`
   - Define scoring profiles
   - Configure evaluation criteria

2. **Add Evaluation Models**
   - Go to `Models â†’ Add`
   - Input API endpoints/credentials
   - Set evaluation parameters

3. **Build Prompt Library**
   - Use `Prompts â†’ New`
   - Apply scoring profiles
   - Bulk import existing sets

4. **Run Evaluations**
   - Start evaluation session
   - Real-time scoring updates
   - Interactive result validation

5. **Analyze Results**
   - Tier classification view
   - Model comparison tools
   - Export detailed reports

## ğŸ¤ Contribution

We welcome contributions!  
ğŸ“Œ First time? Try `good first issue` labeled tickets  
ğŸ”§ Core areas needing help:
- Evaluation workflow enhancements
- Additional storage backends
- Advanced visualization
- CI/CD pipeline improvements

**Contribution Process**:
1. Fork repository
2. Create feature branch
3. Submit PR with description
4. Address review comments
5. Merge after approval

## ğŸ—º Roadmap

### Q2 2025
- ğŸ§  Multi-LLM consensus scoring
- ğŸŒ Distributed evaluation mode
- ğŸ” Advanced search syntax

### Q3 2025
- ğŸ“Š Custom metric support
- ğŸ¤– Auto-evaluation agents
- ğŸ”„ CI/CD integration

## ğŸ“œ License

MIT License - See [LICENSE](LICENSE) for details

## ğŸ“¬ Contact

My work email: [cariyaputta@gmail.com](mailto:cariyaputta@gmail.com)  
