# ğŸ† LLM Tournament Arena

**A comprehensive benchmarking platform for evaluating and comparing Large Language Models**  
*Real-time scoring â€¢ Test suite management â€¢ Collaborative evaluation â€¢ Advanced analytics*

ğŸ“¦ **Single Binary Deployment** â€¢ âš¡ **WebSocket Real-Time Updates** â€¢ ğŸ“Š **Interactive Dashboards**

<details>
    <summary>Program Screenshots (expand)</summary>

UI Results:
![UI Results](./assets/ui-results.png)
UI Evaluate:
![UI Evaluate](./assets/ui-evaluate.png)
UI Stats:
![UI Stats](./assets/ui-stats.png)
UI Prompts:
![UI Prompts](./assets/ui-prompts.png)
UI Edit Prompt:
![UI Edit Prompt](./assets/ui-edit-prompt.png)
UI Profiles:
![UI Profiles](./assets/ui-profiles.png)

</details>

## ğŸš€ Quick Start

```bash
# Clone & Run
git clone https://github.com/lavantien/llm-tournament.git
cd llm-tournament
make run
```

Access at `http://localhost:8080`

## ğŸŒŸ Key Features

### ğŸ§ª **Evaluation Engine**
- ğŸ¯ Real-time scoring with WebSocket updates (0-100 scale with 5 levels)
- ğŸ“ˆ Automatic model ranking with real-time leaderboard
- ğŸ§® Granular scoring system (0/5, 1/5, 2/5, 3/5, 4/5, 5/5)
- ğŸ“‰ Pass percentage calculations and visualization
- ğŸ”„ Instant updates across all connected clients
- ğŸ”€ Random score generation for prototyping
- âª State backup and restore functionality

### ğŸ“š **Test Suite Management**
- ğŸ—‚ï¸ Create/rename/delete independent prompt suites
- ğŸ”— Isolated profiles and results per suite
- âš¡ One-click suite switching with instant UI updates
- ğŸ“¦ Complete suite export/import (JSON)
- ğŸ·ï¸ Profile-based prompt categorization and filtering

### âœï¸ **Prompt Workshop**
- ğŸ“ Rich Markdown editing with live preview
- ğŸ–‡ï¸ Profile assignment for prompt categorization
- ğŸ§© Bulk selection, deletion, and export operations
- ğŸšï¸ Drag-and-drop reordering with automatic saving
- ğŸ” Real-time search and multi-criteria filtering
- ğŸ“‹ One-click copy functionality for prompt text
- ğŸ“¤ JSON export/import with validation

### ğŸ¤– **Model Arena**
- â• Quick model addition with automatic score initialization
- âœï¸ In-place model renaming with result preservation
- ğŸ—‘ï¸ Model deletion with confirmation
- ğŸ“Š Color-coded scoring visualization (red to blue gradient)
- ğŸ”„ Consistent state persistence across sessions
- ğŸ” Model search and filtering capabilities

### ğŸ‘¤ **Profile System**
- ğŸ“‹ Create reusable evaluation profiles
- ğŸ”– Associate profiles with prompts for categorization
- ğŸ”„ Automatic prompt updates when profiles are renamed
- ğŸ” Profile-based filtering in prompt view
- ğŸ“ Markdown description support with preview

### ğŸ“Š **Analytics Suite**
- ğŸ“Š Detailed score breakdowns with Chart.js visualizations
- ğŸ† Comprehensive tier classification system:
  - Transcendent (1900-2000) ğŸŒŒ
  - Super-Grandmaster (1800-1899) ğŸŒŸ
  - Grandmaster (1700-1799) ğŸ¥‡
  - International Master (1600-1699) ğŸ–ï¸
  - Master (1500-1599) ğŸ…
  - Expert (1400-1499) ğŸ“
  - Pro Player (1200-1399) ğŸ®
  - Advanced Player (1000-1199) ğŸ¯
  - Intermediate Player (800-999) ğŸ“ˆ
  - Veteran (600-799) ğŸ‘¨â€ğŸ’¼
  - Beginner (0-599) ğŸ£
- ğŸ“ˆ Score distribution visualization
- ğŸ“‹ Tier-based model grouping
- ğŸ“‘ Performance comparison across models

### ğŸ’» **Evaluation Interface**
- ğŸ¯ Streamlined scoring with color-coded buttons
- ğŸ“ Full prompt and solution display with Markdown rendering
- â¬…ï¸â¡ï¸ Previous/Next navigation between prompts
- ğŸ“‹ One-click copying of raw prompt text
- ğŸ” Clear visualization of current scores
- ğŸƒâ€â™‚ï¸ Rapid evaluation workflow

### ğŸ”„ **Real-Time Collaboration**
- ğŸŒ WebSocket-based instant updates across all clients
- ğŸ“¤ Simultaneous editing with conflict resolution
- ğŸ”„ Broadcast of all changes to connected users
- ğŸ“¡ Connection status monitoring
- ğŸ”„ Automatic reconnection handling

## ğŸ› ï¸ Tech Stack

**Backend**  
`Go 1.21+` â€¢ `Gorilla WebSocket` â€¢ `Blackfriday` â€¢ `Bluemonday`

**Frontend**  
`HTML5` â€¢ `CSS3` â€¢ `JavaScript ES6+` â€¢ `Chart.js 4.x` â€¢ `Marked.js`

**Data**  
`JSON Storage` â€¢ `File-based Persistence` â€¢ `JSON Import/Export` â€¢ `State Versioning`

**Security**  
`XSS Sanitization` â€¢ `CORS Protection` â€¢ `Input Validation` â€¢ `Error Handling`

## ğŸ§° Complementary Tools

**Text-to-Speech**  
`tools/tts/podcast.py` - Generate podcast audio from text scripts using Kokoro ONNX models

**Background Removal**  
`tools/bg_batch_eraser/main.py` - Remove backgrounds from images using BEN2 model  
`tools/bg_batch_eraser/vidseg.py` - Extract foreground from videos with alpha channel support  
`tools/bg_batch_eraser/BEN2.py` - Core background eraser neural network implementation

**LLM Integration**  
`tools/openwebui/pipes/anthropic_claude_thinking_96k.py` - OpenWebUI pipe for Claude with thinking mode (96k context)  
`tools/ragweb_agent` - RAG capabilities for web-based content

## ğŸ Getting Started

### Prerequisites
- Go 1.21+
- Make
- Git

### Installation & Running
```bash
# Development mode
./dev.sh

# Production build
make build
./release/llm-tournament
```

## ğŸ“š Usage Guide

1. **Set Up Test Suites**
   - Create a new suite for your evaluation task
   - Configure profiles for different prompt categories
   - Import existing prompts or create new ones

2. **Configure Models**
   - Add each model you want to evaluate
   - Models can represent different LLMs, versions, or configurations

3. **Prepare Prompts**
   - Write prompts with appropriate solutions
   - Assign profiles for categorization
   - Arrange prompts in desired evaluation order

4. **Run Evaluations**
   - Navigate through prompts and assess each model
   - Use the 0-5 scoring system (0, 20, 40, 60, 80, 100 points)
   - Copy prompts directly to your LLM for testing

5. **Analyze Results**
   - View the results page for summary scores
   - Examine tier classifications in the stats page
   - Compare performance across different prompt types
   - Export results for external analysis

## ğŸ”§ Advanced Features

- **Bulk Operations**: Select multiple prompts for deletion or other actions
- **Drag-and-Drop**: Reorder prompts with intuitive drag-and-drop interface
- **State Preservation**: Previous state can be restored with the "Previous" button
- **Mock Data**: Generate random scores to prototype and test visualizations
- **Search & Filter**: Find specific prompts, models, or profiles quickly

## ğŸ¤ Contribution

We welcome contributions!  
ğŸ“Œ First time? Try `good first issue` labeled tickets  
ğŸ”§ Core areas needing help:
- Evaluation workflow enhancements
- Additional storage backends
- Advanced visualization
- CI/CD pipeline improvements

**Contribution Process**:
1. Fork repository
2. Create feature branch
3. Submit PR with description
4. Address review comments
5. Merge after approval

## ğŸ—º Roadmap

### Q2 2025
- ğŸ§  Multi-LLM consensus scoring
- ğŸŒ Distributed evaluation mode
- ğŸ” Advanced search syntax
- ğŸ“± Responsive mobile design

### Q3 2025
- ğŸ“Š Custom metric definitions
- ğŸ¤– Auto-evaluation agents
- ğŸ”„ CI/CD integration
- ğŸ” User authentication

## ğŸ“œ License

MIT License - See [LICENSE](LICENSE) for details

## ğŸ“¬ Contact

My work email: [cariyaputta@gmail.com](mailto:cariyaputta@gmail.com)
